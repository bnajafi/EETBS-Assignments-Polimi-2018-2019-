{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment10_HCordoba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we are going to export all the assignment 9 in order to start with Assignment 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the external files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ExternalFilesFolder = r\"C:\\Users\\gelli\\Desktop\\POLIMI FIRST YEAR\\ENERGY AND ENVIRONMENTAL SYSTEM FOR BUILDINGS\\Files solved in Python\\Week 9\"\n",
    "ConsumptionFileName= \"consumption_5545.csv\"\n",
    "TemperatureFileName= \"Austin_weather_2014.csv\"\n",
    "IrradianceFileName= \"irradiance_2014_gen.csv\"\n",
    "\n",
    "path_consumptionFile = os.path.join(ExternalFilesFolder,ConsumptionFileName)\n",
    "path_TemperatureFile = os.path.join(ExternalFilesFolder,TemperatureFileName)\n",
    "path_IrradianceFile = os.path.join(ExternalFilesFolder,IrradianceFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets import all our commands from Assignment9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DF_consumption = pd.read_csv(path_consumptionFile,sep=\",\", index_col=0)\n",
    "DF_consumption.head()\n",
    "DF_consumption.tail(10)\n",
    "\n",
    "\n",
    "#We transform the data in the file from 'undefined objects' to 'time and date data'\n",
    "PreviousIndex = DF_consumption.index\n",
    "NewParsedIndex= pd.to_datetime(PreviousIndex)\n",
    "DF_consumption.index =NewParsedIndex \n",
    "\n",
    "DF_consumption.head()\n",
    "DF_consumption.index.hour\n",
    "DF_consumption.index.month\n",
    "DF_consumption.index.dayofweek\n",
    "\n",
    "\n",
    "#We extract/select the data we are interested in: in this case from 2014-07-01 00:00:00 to 2014-07-03 23:00:00\n",
    "DF_consumption_someDaysInJuly=DF_consumption[\"2014-07-01 00:00:00\":\"2014-07-03 23:00:00\"]\n",
    "\n",
    "\n",
    "#We plot the AC Power (in W) consumption versus time during the period we are considering.\n",
    "plt.figure()\n",
    "plt.plot(DF_consumption_someDaysInJuly)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"AC Power (W)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#This is a second way of doing it (better than the other):\n",
    "plt.figure()\n",
    "DF_consumption_someDaysInJuly.plot()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"AC Power (W)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Now we import the weather data, specifically temperature\n",
    "DF_weather = pd.read_csv(path_TemperatureFile,sep=\";\",index_col=0)\n",
    "DF_weather.head(24)\n",
    "\n",
    "#We transform the data in the file from 'undefined objects' to 'time and date data'\n",
    "previousIndex_weather=DF_weather.index\n",
    "newIndex_weather=pd.to_datetime(previousIndex_weather)\n",
    "DF_weather.index = newIndex_weather\n",
    "DF_weather.columns\n",
    "Series_Temperature = DF_weather[\"temperature\"]\n",
    "DF_Temperature= DF_weather[[\"temperature\"]]\n",
    "DF_Temperature.head()\n",
    "\n",
    "\n",
    "#Now we import the weather data, specifically irradiance\n",
    "DF_irradianceSource = pd.read_csv(path_IrradianceFile,sep=\";\",index_col=1)\n",
    "DF_irradianceSource.head(24)\n",
    "DF_irradiance=DF_irradianceSource[[\"gen\"]]\n",
    "DF_irradiance.head(24)\n",
    "\n",
    "\n",
    "#We change the generated irradiance less than 0 into 0 value\n",
    "DF_irradiance[\"gen\"]<0\n",
    "DF_irradiance[DF_irradiance[\"gen\"]<0] = 0\n",
    "DF_irradiance.head(24)\n",
    "\n",
    "\n",
    "#We join the weather data in the same array\n",
    "DF_joined = DF_consumption.join([DF_Temperature,DF_irradiance])\n",
    "DF_joined.head(24)\n",
    "\n",
    "#We remove missing values using .dropna()\n",
    "DF_joined_cleaned = DF_joined.dropna()\n",
    "DF_joined_cleaned.head(24)\n",
    "\n",
    "\n",
    "#We have the pick of temperature that was a 7h which is wrong. The reason is that we are using the time GTM from Greenwhich handling data from Texas. So, we change it just for one day, normalizing as follows:\n",
    "DF_joined_cleaned_copy = DF_joined.dropna().copy()\n",
    "DF_joined_cleaned_chosenDates = DF_joined_cleaned_copy[\"2014-08-01\":\"2014-08-04\"]\n",
    "\n",
    "#Solving the problem with the timezone\n",
    "DF_joined_cleaned_chosenDates[\"temperature\"]=DF_joined_cleaned_chosenDates[\"temperature\"].shift(-5)\n",
    "DF_joined_cleaned_chosenDates.dropna()\n",
    "DF_joined_cleaned_chosenDates.head()\n",
    "DF_joined_cleaned_chosenDates.describe()\n",
    "\n",
    "DF_joined_cleaned_chosenDates_min=DF_joined_cleaned_chosenDates.min()\n",
    "DF_joined_cleaned_chosenDates_max=DF_joined_cleaned_chosenDates.max()\n",
    "DF_joined_cleaned_chosenDates_normalized= (DF_joined_cleaned_chosenDates-DF_joined_cleaned_chosenDates_min)/(DF_joined_cleaned_chosenDates_max-DF_joined_cleaned_chosenDates_min)\n",
    "plt.figure()\n",
    "DF_joined_cleaned_chosenDates_normalized.plot()\n",
    "\n",
    "\n",
    "#Now we want to create lagged features.\n",
    "#First, we reconstruct the dataframe by appliying time-zone\n",
    "DF_joined = DF_consumption.join([DF_Temperature,DF_irradiance])\n",
    "DF_mod = DF_joined.copy()\n",
    "\n",
    "#We have a delay of 5 hours because of time-zoning. We correct it:\n",
    "DF_mod[\"temperature\"]=DF_mod[\"temperature\"].shift(-5)\n",
    "DF_mod.dropna(inplace=True) # It is the same as writing it like this: DF_mod=DF_mod.dropna()\n",
    "\n",
    "#In order to consider the time delays, we add .shift() in temperature array:\n",
    "#DF_mod.head()\n",
    "#DF_mod.describe()\n",
    "\n",
    "#DF_mod[\"Temperature -1h\"]= DF_mod[\"temperature\"].shift(1)\n",
    "#DF_mod[\"Temperature -2h\"]= DF_mod[\"temperature\"].shift(2)\n",
    "#DF_mod[\"Temperature -3h\"]= DF_mod[\"temperature\"].shift(3)\n",
    "#DF_mod[\"Temperature -4h\"]= DF_mod[\"temperature\"].shift(4)\n",
    "#DF_mod[\"Temperature -5h\"]= DF_mod[\"temperature\"].shift(5)\n",
    "#DF_mod[\"Temperature -6h\"]= DF_mod[\"temperature\"].shift(6)\n",
    "#DF_mod.head()\n",
    "\n",
    "#A better way to consider and solve the time delays with the other parameters (such as irradiance and AC Power consumption) is creating a function that includes a bucle for:\n",
    "#First we create a range, setting the array lenght\n",
    "lag_start=1\n",
    "lag_end = 6\n",
    "lag_interval=1\n",
    "column_name=\"temperature\"\n",
    "df=DF_mod\n",
    "\n",
    "#Then, we create the bucle\n",
    "for i in range(lag_start,lag_end+1,lag_interval):\n",
    "    new_column_name = column_name+\" -\"+str(i)+\"hr\"\n",
    "    print new_column_name\n",
    "    df[new_column_name]=df[column_name].shift(i)   \n",
    "    df.dropna(inplace=True) #this removes all the row with a Nan\n",
    "\n",
    "\n",
    "#And finally we create the function including for and .shift() commands\n",
    "def lag_feature(df,column_name,lag_start,lag_end,lag_interval):\n",
    "    for i in range(lag_start,lag_end+1,lag_interval):\n",
    "        new_column_name = column_name+\" -\"+str(i)+\"hr\"\n",
    "        print new_column_name\n",
    "        df[new_column_name]=df[column_name].shift(i)   \n",
    "        df.dropna(inplace=True) #this removes all the row with a Nan\n",
    "    return df\n",
    "\n",
    "\n",
    "#We check it works with and example\n",
    "DF_modified=lag_feature(DF_mod,\"temperature\",1,6,1)\n",
    "DF_modified.head()\n",
    "\n",
    "#We do the same for the irradiance and consumption\n",
    "#We renaming the column names, there are two ways:\n",
    "\n",
    "#DF_mod.columns =[\"AC_consumption\",\"temperature\",\"irradiance\"]\n",
    "#The second way of doing this:\n",
    "DF_mod=DF_mod.rename(columns={\"air conditioner_5545\":\"AC_consumption\",\"gen\":\"irradiance\"})\n",
    "\n",
    "#We use the function for temperature, just for 1 to 6 hours:\n",
    "DF_mod= lag_feature(DF_mod,\"temperature\",1,6,1)\n",
    "DF_mod.head()\n",
    "\n",
    "#We use the function for irradiance, just for 3 to 6 hours:\n",
    "DF_mod= lag_feature(DF_mod,\"irradiance\",3,6,1)\n",
    "\n",
    "#We add the previous consumptions in the last 24 hours!\n",
    "DF_mod=  lag_feature(DF_mod,\"AC_consumption\",1,24,1)\n",
    "DF_mod.head()\n",
    "DF_mod.describe()\n",
    "\n",
    "#Now we add the seasonality parameters (time-related parameters)\n",
    "DF_mod[\"hour\"]=DF_mod.index.hour\n",
    "DF_mod[\"hour\"].head()\n",
    "DF_mod[\"sin_hour\"]=np.sin(DF_mod.index.hour*2*np.pi/24)\n",
    "DF_mod[\"cos_hour\"]=np.cos(DF_mod.index.hour*2*np.pi/24)\n",
    "\n",
    "DF_mod[\"day_of_week\"]=DF_mod.index.dayofweek\n",
    "DF_mod[[\"hour\",\"sin_hour\",\"cos_hour\",\"day_of_week\"]].head(24)\n",
    "\n",
    "DF_mod[\"month\"]=DF_mod.index.month\n",
    "\n",
    "DF_mod[\"week_of_year\"]=DF_mod.index.week\n",
    "\n",
    "\n",
    "#We create a function that recognises weekends:\n",
    "def WeekendDetector(day):\n",
    "    if (day==5 or day == 6):\n",
    "        weekendLabel=1\n",
    "    else:\n",
    "        weekendLabel=0\n",
    "    return weekendLabel\n",
    "\n",
    "\n",
    "#And we apply the function with an example\n",
    "DF_mod[\"weekend\"]= DF_mod[\"day_of_week\"].apply(WeekendDetector)\n",
    "\n",
    "#We create a function that detects working time between 9am-7pm hours  \n",
    "def DayDetector(hour):\n",
    "    if (hour< 19 and hour>=9):\n",
    "        DayLabel=1\n",
    "    else:\n",
    "        DayLabel=0\n",
    "    return DayLab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's start with Assignment #10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DF_mod.head()\n",
    "DF_mod.columns\n",
    "DF_mod.corr()\n",
    "DF_mod.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we choose our data frame to be analyzed on the date where we are going to use most the air conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DF_mod=DF_mod[\"2014-03-01\":\"2014-09-30\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We define our target, that is the main variable that we are going to analyze or to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DF_target=DF_mod[\"AC_consumption\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We define our features but we remove the axis 1 with function .drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DF_features=DF_mod.drop(\"AC_consumption\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we divide between train and test subgroups. We add our variables to manage and also we add a random state in order that everybody have the same random values\n",
    "#### X_train and Y_train are the random values that we are going to use for training, X corresponds to features and Y corresponds to target\n",
    "#### X_test and Y_test are the random values that we are going to use for predict, X corresponds to features and Y corresponds to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(DF_features,DF_target,test_size=0.2,random_state=41234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now We start using models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "linear_reg=linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second step will be fitting a model. We add a variable that will predict the model of X test that are the features. Then we apply this prediction to a dataframe and with add the Ytest in order to compare the features with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_reg.fit(X_train,Y_train)\n",
    "\n",
    "predicted_linearReg_split=linear_reg.predict(X_test)\n",
    "predicted_DF_linearReg_split=pd.DataFrame(predicted_linearReg_split,index=Y_test.index,columns=[\"AC_cons_predicted_linear_reg\"]) \n",
    "predicted_DF_linearReg_split=predicted_DF_linearReg_split.join(Y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We uses this to check if the prediction is working, and we select the month of august."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_DF_linearReg_split_august=predicted_DF_linearReg_split[\"2014-08-01\":\"2014-08-31\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's plot to see how similar could be the prediction to the real values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_DF_linearReg_split_august.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we want to estimate how accurate are our predictions and we use  MAE, MSE, R2 functions of prediction. Again we import everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure of difference between two continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAE_linearReg_split=mean_absolute_error(predicted_linearReg_split,Y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures the average of the squares of the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MSE_linearReg_split=mean_squared_error(predicted_linearReg_split,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficient of determination regression score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R2_linearReg_split=r2_score(predicted_linearReg_split,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second way of doing this is using k-folf cross validation\n",
    "##### cross validation is another way of predicting learning to compare and select a model for a given predictive modeling problem \n",
    "#### We follow the same steps that we did for split but with other method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "predicted_linearReg_CV=cross_val_predict(linear_reg,DF_features,DF_target,cv=10) #cv=10 porque dividimos en 10 subgrupos\n",
    "predicted_DF_linearReg_CV=pd.DataFrame(predicted_linearReg_CV,index=DF_target.index,columns=[\"AC_cons_predicted_linear_CV\"])\n",
    "predicted_DF_linearReg_CV=predicted_DF_linearReg_CV.join(DF_target) #.join() matches index = matches the data\n",
    "\n",
    "predicted_DF_linearReg_CV_august=predicted_DF_linearReg_CV[\"2014-08-01\":\"2014-08-31\"]\n",
    "predicted_DF_linearReg_CV_august.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we calculate again how accurate where the predictions with this new method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "\n",
    "MAE_linearReg_CV=mean_absolute_error(predicted_linearReg_CV,DF_target)\n",
    "MSE_linearReg_CV=mean_squared_error(predicted_linearReg_CV,DF_target)\n",
    "R2_linearReg_CV=r2_score(predicted_linearReg_CV,DF_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we try other algorithm\n",
    "#### The third way of doing this is using RandomForestRegressor\n",
    "##### meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging and select a model for a given predictive modeling problem \n",
    "#### We follow the same steps that we did for split but with other method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg_RF=RandomForestRegressor()\n",
    "\n",
    "predicted_RF_CV=cross_val_predict(reg_RF,DF_features,DF_target,cv=10) #cv=10 porque dividimos en 10 subgrupos\n",
    "predicted_DF_RF_CV=pd.DataFrame(predicted_RF_CV,index=DF_target.index,columns=[\"AC_cons_predicted_RF_CV\"])\n",
    "predicted_DF_RF_CV=predicted_DF_RF_CV.join(DF_target) #.join() matches index = matches the data\n",
    "\n",
    "predicted_DF_RF_CV_august=predicted_DF_RF_CV[\"2014-08-01\":\"2014-08-31\"]\n",
    "predicted_DF_RF_CV_august.plot()\n",
    "\n",
    "MAE_RF_CV=mean_absolute_error(predicted_RF_CV,DF_target)\n",
    "MSE_RF_CV=mean_squared_error(predicted_RF_CV,DF_target)\n",
    "R2_RF_CV=r2_score(predicted_RF_CV,DF_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we want to use online learning.We take our data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DF_onlineConsumptionPrediction=pd.DataFrame(index=DF_mod.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We select a period of training that would be 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "period_of_training=pd.Timedelta(30,unit=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With this variable, we apply the first date where we are going to start the model for AC consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FirstTimeStamp_measured=DF_mod.index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This variable represent the last date of your model for AC consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LastTimeStamp_measured=DF_mod.index[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firsttime to predict is going to be after 30 days of the starting day that is our firstTimeStamp_measured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FirstTimeStamp_toPredict=FirstTimeStamp_measured+period_of_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our first day to training is going to start in our first date for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_startTimeStamp=FirstTimeStamp_measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our training is going to finish when it reaches this 30 days of training for every month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_endTimeStamp=FirstTimeStamp_toPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first day to predict is going to start when the training ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timeStamp_toPredict=FirstTimeStamp_toPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Before training_endTimeStamp, remove everything for the consumption prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DF_onlineConsumptionPrediction.truncate(before=training_endTimeStamp)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we are going to do a condition for our model, between training and prediction With this while, we can use any of the previous model to do the predictions, but with the while instead of changing and predicted for days\n",
    "#### We are training first for the 30 days and the we are predicting every hour, so we choose the data of the previous month and then we predict the next hour. For doing this procedures of taking the previous as training and the predict the next hour, \n",
    "#### We are changing the variable by adding them a timedelta of one hour, so everytime, we are going to change our training and test for our features and also for our target and all this loop is starting from 1st of March until he finnally recognize that we are in the end of.Our model that is the last day of the month of september.\n",
    "\n",
    "#### Useful to use the data of the previous month because. It is good to predict and also because is recent so it is better to do it one month before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while (timeStamp_toPredict<LastTimeStamp_measured):\n",
    "    \n",
    "    DF_feature_train=DF_features.truncate(before=training_startTimeStamp,after=training_endTimeStamp) #.truncate() elimina todo lo que no estÃ© comprendido entre los valores before-after\n",
    "    DF_target_train=DF_target.truncate(before=training_startTimeStamp,after=training_endTimeStamp)\n",
    "    DF_feature_test=DF_features.loc[timeStamp_toPredict].values.reshape(1,-1) #we need to add it, otherwise it gives error\n",
    "    DF_target_test=DF_target.loc[timeStamp_toPredict]\n",
    "    reg_RF.fit(DF_feature_train,DF_target_train) #this is for traininig with the training data\n",
    "    predicted_Consumption=reg_RF.predict(DF_feature_test)\n",
    "    DF_onlineConsumptionPrediction.loc[timeStamp_toPredict,\"Predicted\"]=predicted_Consumption\n",
    "    DF_onlineConsumptionPrediction.loc[timeStamp_toPredict,\"Real\"]=DF_target_test\n",
    "\n",
    "    timeStamp_toPredict=timeStamp_toPredict+pd.Timedelta(1,unit=\"h\") \n",
    "    training_endTimeStamp=training_endTimeStamp+pd.Timedelta(1,unit=\"h\")\n",
    "    training_startTimeStamp=training_startTimeStamp+pd.Timedelta(1,unit=\"h\")\n",
    "\n",
    "\n",
    "DF_onlineConsumptionPrediction.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we calculate how accurate is with the online learning for model prediction Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R2_score_online_RF=r2_score(DF_onlineConsumptionPrediction[[\"Real\"]],DF_onlineConsumptionPrediction[[\"Predicted\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
